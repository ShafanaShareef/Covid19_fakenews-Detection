{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4opl-1dttmi2"},"outputs":[],"source":["!pip install pydub\n","!pip install SpeechRecognition\n","!pip install google-auth google-auth-oauthlib google-auth-httplib2\n","!pip install google-cloud-translate\n","!pip install webrtcvad\n","!pip install scipy\n","!pip install nltk\n","!pip install AudioSegment\n","!pip install google-cloud-speech\n","!pip install googletrans==3.1.0a0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJHbluZttu9c"},"outputs":[],"source":["import speech_recognition as sr\n","from pydub import AudioSegment\n","import webrtcvad\n","import numpy as np\n","import google.auth\n","from google.cloud import speech_v1\n","from google.cloud import translate_v2 as translate\n","import scipy\n","from scipy.io import wavfile\n","import googletrans\n","googletrans.__version__\n","import nltk\n","\n","from googletrans import Translator\n","import pandas as pd\n","\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.util import ngrams\n","from collections import Counter\n","from nltk import pos_tag, word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"livUSeZQt09s"},"outputs":[],"source":["translator=Translator()\n","\n","# Example dataset of news articles\n","data =  pd.read_csv('/content/News.csv', encoding='latin-1')\n","\n","trueArticles = data[data['Binary Label'] == 1]['Text']\n","fakeArticles = data[data['Binary Label'] == 0]['Text']\n","# news_articles = data['Text']\n","# print(news_articles);\n","# print(trueArticles)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dd55hNy7Qiq5"},"outputs":[],"source":["print (data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e43YOz7fQuT6","outputId":"32f72ca8-e722-4e08-ad2a-60715ef87e00"},"outputs":[{"data":{"text/plain":["(7591, 5)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1LkDgPvQzH9"},"outputs":[],"source":["data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnUXKmmuQ4Cj"},"outputs":[],"source":["data = data.fillna('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcF4fX9xQ7qH"},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UiGTPeX0t4bH"},"outputs":[],"source":["def transcribe_audio(audio_path,lang):\n","# Load audio file\n","  sound = AudioSegment.from_wav(audio_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"E6Wunlkot9FC"},"outputs":[],"source":["def transcribe_audio(audio_path, lang):\n","    if lang == 'Sinhala':\n","        # Convert audio to text using Google Cloud Speech API\n","        r = sr.Recognizer()\n","        with sr.AudioFile(audio_path) as source:\n","            audio = r.record(source)\n","        text = r.recognize_google(audio, language='si-LK')\n","        print(text)\n","        # translator = translate.Translator()\n","        result = translator.translate(text)\n","        text = result.text\n","        print(text)\n","        # Print the transcribed text\n","        print(\"Transcribed Text: \", text)\n","\n","    elif lang == 'Tamil':\n","        # Convert audio to text using Google Cloud Speech API\n","        r = sr.Recognizer()\n","        with sr.AudioFile(audio_path) as source:\n","            audio = r.record(source)\n","        text = r.recognize_google(audio, language='ta-LK')\n","        print(text)\n","        # translator = translate.Translator()\n","        result = translator.translate(text)\n","        text = result.text\n","        print(text)\n","        # Print the transcribed text\n","        print(\"Transcribed Text: \", text)\n","\n","    else:\n","        # Convert audio to text using Google Cloud Speech API\n","        r = sr.Recognizer()\n","        with sr.AudioFile(audio_path) as source:\n","            audio = r.record(source)\n","        text = r.recognize_google(audio)\n","\n","        # Print the transcribed text\n","        print(\"Transcribed Text: \", text)\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKg3HBtTt987"},"outputs":[],"source":["def translate_text(text, target_language=\"en\"):\n","    from google.cloud import translate\n","\n","    # credentials_path = '/content/heroic-equinox-387605-2bf604959d3e.json'  \n","    # translate_client = translate.Client(credentials=credentials_path)\n","    result = translate_client.translate(text, target_language=target_language)\n","\n","    return result['input'], result['translatedText']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmxdVbIKuAXy"},"outputs":[],"source":["def split_array(arr, size):\n","    arrs = []\n","    while len(arr) > size:\n","        piece = arr[:size]\n","        arrs.append(piece)\n","        arr = arr[size:]\n","    arrs.append(arr)\n","    return arrs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghWFfGLjuCwF"},"outputs":[],"source":["def is_human_audio(audio_path):\n","  fs, audio = wavfile.read(audio_path)\n","  audio_n = audio/float(2**15)\n","\n","  vad = webrtcvad.Vad(3)\n","  def audioSlice(x, fs, framesz, hop):\n","      framesamp = int(framesz*fs)\n","      hopsamp = int(hop*fs)\n","      X = scipy.array([x[i:i+framesamp] for i in range(0, len(x)-framesamp, hopsamp)])\n","      return X\n","  framesz=10./1000 #10 ms \n","  hop = 1.0*framesz\n","  Z = audioSlice(audio_n, fs, framesz, hop)\n","  fr = np.int16(Z[100] * 32768).tobytes()\n","  aud = vad.is_speech(fr, fs)  \n","  if (aud):\n","      print(\"Human Voice \")\n","      return True\n","  else:\n","      print(\"Generated Voice\")\n","      return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6tpFoguuFGJ"},"outputs":[],"source":["\n","if __name__ == \"__main__\":\n","    \n","   # Transcribe the audio to text\n","    lang = input(\"Is This English or Sinhala or Tamil.?\")\n","    \n","    if lang=='Sinhala':\n","      audio_path = \"/content/Sinhala voice.wav\"\n","    elif lang=='Tamil':\n","      audio_path = \"/content/Tamil voice.wav\"\n","    else:\n","      audio_path = \"/content/English voice.wav\"\n","    text = transcribe_audio(audio_path,lang)\n","    \n","    # Detect if the audio is human or generated\n","    is_human = is_human_audio(audio_path)\n","  \n","    print(f\"Input Text: {text}\")   \n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fw2tYMG3uH4q"},"outputs":[],"source":["n_val_of_text=0\n","n_val_of_article=0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhAdWWbEuKqG"},"outputs":[],"source":["def preprocess(text, n):\n","    global n_val_of_text\n","    global n_val_of_article\n","\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Tokenize text into words\n","    words = word_tokenize(text)\n","\n","    # Remove stopwords and punctuation\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word.isalnum() and word not in stop_words]\n","    \n","    pos = pos_tag(words)\n","    \n","    # Extract nouns and adjectives\n","    nouns_adj = [word[0] for word in pos if word[1] in ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']]\n","    \n","    # Take ngrams from text\n","    ngram_list = ngrams(text.split(' '), n)\n","    nlist = list(ngram_list)\n","\n","    # Adding nouns and adjectives to entire ngram utilizing parts of speach\n","    nadjtup = tuple(nouns_adj)\n","    nlist.append(nadjtup)\n","  \n","    return nlist\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDF9goXWuTQ6"},"outputs":[],"source":["# Preprocess each news article and store n-grams\n","\n","def new_compare(ngramFrequency, textng):\n","    score = 0\n","    for ngs, count in ngramFrequency.items():\n","      for ang in textng:\n","        check = all(item in ngs for item in ang) # check all items in a ngram of text contain in a ngram of article text\n","        if check:\n","          score += count\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pr9oekjvlk91","outputId":"50471435-a310-40e1-c3f1-9c6d1009bafd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[(50, 0), (25, 0), (25, 0)]\n","[50, 25, 25]\n","50 50\n","Accuracy : \n","100.0%\n","This is a Real News.\n"]}],"source":["# text='coronavirus is bioengineered. it may use as bioweapon'\n","# text= 'The coronavirus was first discovered in Wuhan'\n","\n","# Preprocess each news article and store n-grams for different values of n\n","article_ngrams_1gram = []\n","article_ngrams_2gram = []\n","article_ngrams_3gram = []\n","\n","def makeListngram(ng):\n","      nglis=[]\n","      for e in ng:\n","        for ngms in e: #here\n","          nglis.append(ngms)\n","      return nglis\n","\n","def gettingScore(trueArticles, fakeArticles, nN):\n","    text_ngrams = []\n","    # for article3 in text:\n","    d=preprocess(text, nN)\n","\n","    # print(list(d))\n","    text_ngrams.append(d)    \n","\n","    trueArticlesNgrams = []\n","    fakeArticlesNgrams = []\n","\n","    for articleT in trueArticles:\n","      d1 = preprocess(articleT, nN)\n","      trueArticlesNgrams.append(d1)\n","\n","    for articleF in fakeArticles:\n","      d2 = preprocess(articleF, nN)\n","      fakeArticlesNgrams.append(d2)\n","\n","\n","    listOfNgramsTrue = makeListngram(trueArticlesNgrams)\n","    listOfNgramsFake = makeListngram(fakeArticlesNgrams)\n","    # print(listOfNgramsFake[10:])\n","\n","    Truengram_freq = Counter(listOfNgramsTrue)\n","    Fakengram_freq = Counter(listOfNgramsFake)\n","\n","\n","    listedTextngrams = text_ngrams[0] #here\n","    TrueScore = new_compare(Truengram_freq, listedTextngrams)\n","    FakeScore = new_compare(Fakengram_freq, listedTextngrams)\n","\n","    return TrueScore, FakeScore\n","\n","\n","scores = []\n","\n","for i in range(3,6):\n","  retScores = gettingScore(trueArticles, fakeArticles, i)\n","  scores.append(retScores)\n","print(scores)\n","\n","# Getting total true score and total score (true+fake)\n","totalScore=0\n","totals=[]\n","for score in scores:\n","  totals.append(score[0]+score[1])\n","  totalScore += (score[0]+score[1])\n","\n","maxScoreIndex = totals.index(max(totals))\n","BestNgramTrueScore = scores[maxScoreIndex][0]\n","totalScore = max(totals)\n","print(totals)\n","\n","print(BestNgramTrueScore, totalScore)\n","if totalScore==0:\n","  print('Cannot predict. insufficient data.')\n","\n","else:\n","  confidence = (BestNgramTrueScore/totalScore)*100\n","  print('Accuracy : ')\n","  print(str(confidence)+'%')\n","  if confidence>50:\n","    print('This is a Real News.')\n","\n","  else:\n","    print('This is a Fake News.')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}